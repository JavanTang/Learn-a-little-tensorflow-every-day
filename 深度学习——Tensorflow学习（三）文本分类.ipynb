{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "深度学习——Tensorflow学习（三）文本分类.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JavanTang/Learn-a-little-tensorflow-every-day/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Tensorflow%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "j25r9NELJe_U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "简单概述一下本次的任务与昨天的任务类似都是做分类，但是我们需要使用tensorflow去做的是文本分类，将文本形式的影评分为正面与负面，我们使用的是的`数据集`是来自IMDB的数据集，之后我们还要下载一些中文的数据进行分析，在官方教程的基础上做扩充。\n",
        "\n",
        "\n",
        "## 任务简述\n",
        "1. 下载IMDB数据集\n",
        "2. 分析数据\n",
        "3. 将数据格式化\n",
        "4. 构建模型\n",
        "5. 训练模型\n",
        "6. 验证模型\n",
        "7. **评估模型**\n",
        "8. 使用中文数据集进行练习\n",
        "\n",
        "这里插一句本系列练习中都有Colaboratory，这个是Google大佬免费提供的机器学习的一个平台，如果有翻墙可以直接点Colaboratory链接，那样学习会更加轻松，排版也会较为舒服而且还可以随时修改参数。\n",
        "\n",
        "\n",
        "## 上代码"
      ]
    },
    {
      "metadata": {
        "id": "H2j26Tp96F9n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 导入包\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as k\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OsltOR8W6Rf8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "imdb = k.datasets.imdb\n",
        "\n",
        "(train_datas, train_labels), (test_datas, test_labels) = imdb.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oYhsOEdi6wgy",
        "colab_type": "code",
        "outputId": "1ad9f848-d6d4-46c3-8e22-71a7a4224534",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(train_datas[0])"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I3KC5S5e7G9R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "我们先查看第一个样本，这个都是数字组成的，那是因为IMDB将文字都化成了数字"
      ]
    },
    {
      "metadata": {
        "id": "92gYWYGy62YS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 文字:数字的对应关系\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# 上面我们只有数字，所以我们现在需要将word_index变成“数字对应文字的关系”\n",
        "index_word = {v:k for k,v in word_index.items()}\n",
        "\n",
        "def decode_review(text):\n",
        "  '''\n",
        "  将text list中的number转换称为word\n",
        "  '''\n",
        "  return ' '.join([index_word.get(i, '?') for i in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0smTQ2gx8gma",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1f6dc986-a4f3-45c6-ac64-24a3ba7b66f1"
      },
      "cell_type": "code",
      "source": [
        "# 使用这个函数可以还原数据\n",
        "print(decode_review(train_datas[0]))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room titillate it so heart shows to years of every never going villaronga help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but pratfalls to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other tricky in of seen over landed for anyone of gilmore's br show's to whether from than out themselves history he name half some br of 'n odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VJrWuEPIH18n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "下面开始构建模型，在构建模型之前我们回顾一下（二）中的图片分类有一个input_shape参数，当时这个参数我们选用了28*28，这是因为像素点矩阵是统一的，同时我们设置的模型内部的节点也是统一的（这句话不理解可以重新看看（一）中对神经网络的理解）\n",
        "\n",
        "这里为了长度标准化我们使用了pad_sequences函数。\n",
        "```\n",
        "函数说明： \n",
        "将长为nb_samples的序列（标量序列）转化为形如(nb_samples,nb_timesteps)2D numpy array。如果提供了参数maxlen，nb_timesteps=maxlen，否则其值为最长序列的长度。其他短于该长度的序列都会在后部填充0以达到该长度。长于nb_timesteps的序列将会被截断，以使其匹配目标长度。padding和截断发生的位置分别取决于padding和truncating. \n",
        "参数：\n",
        "sequences：浮点数或整数构成的两层嵌套列表\n",
        "maxlen：None或整数，为序列的最大长度。大于此长度的序列将被截短，小于此长度的序列将在后部填0.\n",
        "dtype：返回的numpy array的数据类型\n",
        "padding：‘pre’或‘post’，确定当需要补0时，在序列的起始还是结尾补\n",
        "truncating：‘pre’或‘post’，确定当需要截断序列时，从起始还是结尾截断\n",
        "value：浮点数，此值将在填充时代替默认的填充值0\n",
        "返回值： \n",
        "返回形如(nb_samples,nb_timesteps)的2D张量\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DYigHVusKSyY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_datas = k.preprocessing.sequence.pad_sequences(train_datas,\n",
        "                                                    maxlen=256,\n",
        "                                                    padding='post',\n",
        "                                                    value=0\n",
        "                                                        )\n",
        "test_datas = k.preprocessing.sequence.pad_sequences(test_datas,\n",
        "                                                   maxlen=256,\n",
        "                                                   padding='post',\n",
        "                                                   value=0\n",
        "                                                   )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rQB-7wRxMB1B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "81683bdf-99a7-4466-999c-053c74fa4ff5"
      },
      "cell_type": "code",
      "source": [
        "# 通过print我们可以发现后面的都被替换成了-1，同时所有的特征全部变成了256的长度\n",
        "print(train_datas[1])"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[    1   194  1153   194  8255    78   228     5     6  1463  4369  5012\n",
            "   134    26     4   715     8   118  1634    14   394    20    13   119\n",
            "   954   189   102     5   207   110  3103    21    14    69   188     8\n",
            "    30    23     7     4   249   126    93     4   114     9  2300  1523\n",
            "     5   647     4   116     9    35  8163     4   229     9   340  1322\n",
            "     4   118     9     4   130  4901    19     4  1002     5    89    29\n",
            "   952    46    37     4   455     9    45    43    38  1543  1905   398\n",
            "     4  1649    26  6853     5   163    11  3215 10156     4  1153     9\n",
            "   194   775     7  8255 11596   349  2637   148   605 15358  8003    15\n",
            "   123   125    68 23141  6853    15   349   165  4362    98     5     4\n",
            "   228     9    43 36893  1157    15   299   120     5   120   174    11\n",
            "   220   175   136    50     9  4373   228  8255     5 25249   656   245\n",
            "  2350     5     4  9837   131   152   491    18 46151    32  7464  1212\n",
            "    14     9     6   371    78    22   625    64  1382     9     8   168\n",
            "   145    23     4  1690    15    16     4  1355     5    28     6    52\n",
            "   154   462    33    89    78   285    16   145    95     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dVEhl0UrMx78",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 构建模型, 这里我们先自行DIV一下\n",
        "# 先用（二）中图片分类的模型试试看\n",
        "\n",
        "model = k.Sequential([\n",
        "#     这个不用，具体原因可以看（一）中的解释\n",
        "#     keras.layers.Flatten(input_shape=(28,28)),\n",
        "    k.layers.Dense(128, activation=tf.nn.relu),\n",
        "    k.layers.Dense(2, activation=tf.nn.softmax)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WQxhukAWvQKh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ljyBt_qWvTQo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1823
        },
        "outputId": "97be774b-d829-4c09-82ed-af52c76f4da8"
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(train_datas,\n",
        "                    train_labels,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    verbose=1)\n"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-200-821a6026c731>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     verbose=1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# Setup work for each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'metrics'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0mtraining_distributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m     \"\"\"\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m       \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mset_value\u001b[0;34m(x, value)\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2846\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2847\u001b[0;31m       \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    480\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m       \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "vZm11ZsbsyWD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "准确性全部是50%，五五开？？？\n",
        "\n",
        "这里我们需要思考，为什么将一句话放进去出来的结果确是这样？在这个系列的学习中，主要是要学会思考，学会寻找问题并解决问题。\n",
        "\n",
        "我们来想想train_datas中的数据是什么样的，比如取第一条数据可能是:[\"223\".\"13\",\"3\",\"22\",\"19\",...,...]，其中这些数字是word_index中的序号，这些序号放入神经网络中为什么是五五开？\n",
        "\n",
        "给一下几个方向：\n",
        "\n",
        "1. 训练数据\n",
        "2. 设计的模型\n",
        "3. 损失函数\n",
        "\n",
        "训练数据：\n",
        "\n",
        "这个方面主要是我们数据的特征没有提取好，那我们如何解决这个问题？\n",
        "**Google 或者 Bing 或者 Baidu 搜索：神经网络 文本如何提取特征**，然后研读前5篇文章，这也是本次的练习，将五篇的文章总结一下，总结使用什么方法。\n",
        "\n",
        "在官方的教程中它直接直接使用了keras.layers.Embedding去提取了词向量。\n",
        "\n",
        "在那五篇文章中没有看到关于词向量的解释，可以看看这些[知乎中对词向量的解释](https://www.zhihu.com/question/21714667)，[Embedding](https://blog.csdn.net/wangyangzhizhou/article/details/77530479)\n",
        "\n",
        "那我们改写一下模型："
      ]
    },
    {
      "metadata": {
        "id": "K9mIMVcF8lIX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "6d85b330-3476-4131-cd00-6d6fbaaf3fcc"
      },
      "cell_type": "code",
      "source": [
        "train_datas[0]"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    1,    14,    22,    16,    43,   530,   973,  1622,  1385,\n",
              "          65,   458,  4468,    66,  3941,     4,   173,    36,   256,\n",
              "           5,    25,   100,    43,   838,   112,    50,   670, 22665,\n",
              "           9,    35,   480,   284,     5,   150,     4,   172,   112,\n",
              "         167, 21631,   336,   385,    39,     4,   172,  4536,  1111,\n",
              "          17,   546,    38,    13,   447,     4,   192,    50,    16,\n",
              "           6,   147,  2025,    19,    14,    22,     4,  1920,  4613,\n",
              "         469,     4,    22,    71,    87,    12,    16,    43,   530,\n",
              "          38,    76,    15,    13,  1247,     4,    22,    17,   515,\n",
              "          17,    12,    16,   626,    18, 19193,     5,    62,   386,\n",
              "          12,     8,   316,     8,   106,     5,     4,  2223,  5244,\n",
              "          16,   480,    66,  3785,    33,     4,   130,    12,    16,\n",
              "          38,   619,     5,    25,   124,    51,    36,   135,    48,\n",
              "          25,  1415,    33,     6,    22,    12,   215,    28,    77,\n",
              "          52,     5,    14,   407,    16,    82, 10311,     8,     4,\n",
              "         107,   117,  5952,    15,   256,     4, 31050,     7,  3766,\n",
              "           5,   723,    36,    71,    43,   530,   476,    26,   400,\n",
              "         317,    46,     7,     4, 12118,  1029,    13,   104,    88,\n",
              "           4,   381,    15,   297,    98,    32,  2071,    56,    26,\n",
              "         141,     6,   194,  7486,    18,     4,   226,    22,    21,\n",
              "         134,   476,    26,   480,     5,   144,    30,  5535,    18,\n",
              "          51,    36,    28,   224,    92,    25,   104,     4,   226,\n",
              "          65,    16,    38,  1334,    88,    12,    16,   283,     5,\n",
              "          16,  4472,   113,   103,    32,    15,    16,  5345,    19,\n",
              "         178,    32,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    },
    {
      "metadata": {
        "id": "X5TE4bS-UQMe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "d079dafd-a305-4dca-af3b-0626d353b243"
      },
      "cell_type": "code",
      "source": [
        "vob_len = len(index_word)+1\n",
        "\n",
        "print(vob_len)\n",
        "\n",
        "model = k.Sequential([\n",
        "    k.layers.Embedding(vob_len+2, 16),\n",
        "    k.layers.GlobalAveragePooling1D(),\n",
        "    k.layers.Dense(16, activation=tf.nn.relu),\n",
        "    k.layers.Dense(1, activation=tf.nn.sigmoid)\n",
        "])\n",
        "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 下面这条语句出现的的信息很重要，从这里我们一般就知道我们数据是怎么变化的，同时每一层出来的是什么我们也可以知道。\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "88585\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_36 (Embedding)     (None, None, 16)          1417392   \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_13  (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_106 (Dense)            (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_107 (Dense)            (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 1,417,681\n",
            "Trainable params: 1,417,681\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kSuJNN05JnUe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "在k.layers.Dense中有一个参数是activation，这里叫做激活函数[什么是激活函数](https://www.zhihu.com/question/22334626)\n",
        "\n",
        "这里又留了一个问题：\n",
        "\n",
        "**在keras中有几种激活函数，列举出来，当前我们已经使用过的这几种是什么意思？**\n",
        "\n",
        "\n",
        "\n",
        "上面那个代码大家还没有接触到的应该是`k.layers.GlobalAveragePooling1D`函数，留下几个问题：\n",
        "\n",
        "1. 这个函数的作用是大家的一个练习，自行去找资料去解决问题。\n",
        "2. 同时大家去掉这个来看看会出现什么问题。\n",
        "3. 对于loss与这个系列（一）不同，我这里简单的说一下就是binary_crossentropy，是处理二分类问题，具体的大家可以去网上自行查询，实在有困难可以留言评论。\n",
        "\n",
        "下面继续coding\n"
      ]
    },
    {
      "metadata": {
        "id": "4aCypuiJAW-S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1462
        },
        "outputId": "dfe462a7-565b-42d4-9e73-3e4340a69172"
      },
      "cell_type": "code",
      "source": [
        "#创建了验证集,前10000个做训练，后10000个做验证集\n",
        "#https://www.zhihu.com/question/26588665 这里解释了什么是测试集与验证集的\n",
        "x_val = train_datas[:10000]\n",
        "partial_x_train = train_datas[10000:]\n",
        "\n",
        "y_val = train_labels[:10000]\n",
        "partial_y_train = train_labels[10000:]\n",
        "\n",
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    verbose=1)"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "15000/15000 [==============================] - 2s 111us/sample - loss: 0.2216 - acc: 0.9429 - val_loss: 0.3247 - val_acc: 0.8779\n",
            "Epoch 2/40\n",
            "15000/15000 [==============================] - 2s 113us/sample - loss: 0.1973 - acc: 0.9491 - val_loss: 0.3136 - val_acc: 0.8793\n",
            "Epoch 3/40\n",
            "15000/15000 [==============================] - 2s 111us/sample - loss: 0.1772 - acc: 0.9549 - val_loss: 0.3041 - val_acc: 0.8811\n",
            "Epoch 4/40\n",
            "15000/15000 [==============================] - 2s 113us/sample - loss: 0.1599 - acc: 0.9614 - val_loss: 0.2974 - val_acc: 0.8832\n",
            "Epoch 5/40\n",
            "15000/15000 [==============================] - 2s 110us/sample - loss: 0.1452 - acc: 0.9653 - val_loss: 0.2919 - val_acc: 0.8841\n",
            "Epoch 6/40\n",
            "15000/15000 [==============================] - 2s 109us/sample - loss: 0.1326 - acc: 0.9683 - val_loss: 0.2884 - val_acc: 0.8846\n",
            "Epoch 7/40\n",
            "15000/15000 [==============================] - 2s 111us/sample - loss: 0.1213 - acc: 0.9717 - val_loss: 0.2858 - val_acc: 0.8858\n",
            "Epoch 8/40\n",
            "15000/15000 [==============================] - 2s 113us/sample - loss: 0.1115 - acc: 0.9755 - val_loss: 0.2832 - val_acc: 0.8868\n",
            "Epoch 9/40\n",
            "15000/15000 [==============================] - 2s 112us/sample - loss: 0.1024 - acc: 0.9787 - val_loss: 0.2816 - val_acc: 0.8874\n",
            "Epoch 10/40\n",
            "15000/15000 [==============================] - 2s 108us/sample - loss: 0.0946 - acc: 0.9809 - val_loss: 0.2808 - val_acc: 0.8877\n",
            "Epoch 11/40\n",
            "15000/15000 [==============================] - 2s 110us/sample - loss: 0.0873 - acc: 0.9833 - val_loss: 0.2808 - val_acc: 0.8876\n",
            "Epoch 12/40\n",
            "15000/15000 [==============================] - 2s 108us/sample - loss: 0.0808 - acc: 0.9848 - val_loss: 0.2812 - val_acc: 0.8874\n",
            "Epoch 13/40\n",
            "15000/15000 [==============================] - 2s 109us/sample - loss: 0.0751 - acc: 0.9863 - val_loss: 0.2831 - val_acc: 0.8877\n",
            "Epoch 14/40\n",
            "15000/15000 [==============================] - 2s 112us/sample - loss: 0.0700 - acc: 0.9875 - val_loss: 0.2822 - val_acc: 0.8876\n",
            "Epoch 15/40\n",
            "15000/15000 [==============================] - 2s 111us/sample - loss: 0.0652 - acc: 0.9882 - val_loss: 0.2833 - val_acc: 0.8872\n",
            "Epoch 16/40\n",
            "15000/15000 [==============================] - 2s 107us/sample - loss: 0.0606 - acc: 0.9893 - val_loss: 0.2854 - val_acc: 0.8879\n",
            "Epoch 17/40\n",
            "15000/15000 [==============================] - 2s 108us/sample - loss: 0.0564 - acc: 0.9907 - val_loss: 0.2867 - val_acc: 0.8869\n",
            "Epoch 18/40\n",
            "15000/15000 [==============================] - 2s 107us/sample - loss: 0.0527 - acc: 0.9919 - val_loss: 0.2885 - val_acc: 0.8883\n",
            "Epoch 19/40\n",
            "15000/15000 [==============================] - 2s 110us/sample - loss: 0.0491 - acc: 0.9925 - val_loss: 0.2918 - val_acc: 0.8859\n",
            "Epoch 20/40\n",
            "15000/15000 [==============================] - 2s 112us/sample - loss: 0.0461 - acc: 0.9929 - val_loss: 0.2934 - val_acc: 0.8865\n",
            "Epoch 21/40\n",
            "15000/15000 [==============================] - 2s 113us/sample - loss: 0.0430 - acc: 0.9934 - val_loss: 0.2954 - val_acc: 0.8867\n",
            "Epoch 22/40\n",
            "15000/15000 [==============================] - 2s 112us/sample - loss: 0.0405 - acc: 0.9939 - val_loss: 0.2991 - val_acc: 0.8859\n",
            "Epoch 23/40\n",
            "15000/15000 [==============================] - 2s 108us/sample - loss: 0.0378 - acc: 0.9949 - val_loss: 0.3007 - val_acc: 0.8861\n",
            "Epoch 24/40\n",
            "15000/15000 [==============================] - 2s 110us/sample - loss: 0.0353 - acc: 0.9951 - val_loss: 0.3034 - val_acc: 0.8863\n",
            "Epoch 25/40\n",
            "15000/15000 [==============================] - 2s 111us/sample - loss: 0.0331 - acc: 0.9955 - val_loss: 0.3056 - val_acc: 0.8864\n",
            "Epoch 26/40\n",
            "15000/15000 [==============================] - 2s 111us/sample - loss: 0.0310 - acc: 0.9956 - val_loss: 0.3085 - val_acc: 0.8851\n",
            "Epoch 27/40\n",
            "15000/15000 [==============================] - 2s 111us/sample - loss: 0.0291 - acc: 0.9961 - val_loss: 0.3121 - val_acc: 0.8852\n",
            "Epoch 28/40\n",
            "15000/15000 [==============================] - 2s 109us/sample - loss: 0.0274 - acc: 0.9963 - val_loss: 0.3178 - val_acc: 0.8841\n",
            "Epoch 29/40\n",
            "15000/15000 [==============================] - 2s 112us/sample - loss: 0.0260 - acc: 0.9968 - val_loss: 0.3188 - val_acc: 0.8837\n",
            "Epoch 30/40\n",
            "15000/15000 [==============================] - 2s 111us/sample - loss: 0.0243 - acc: 0.9972 - val_loss: 0.3227 - val_acc: 0.8828\n",
            "Epoch 31/40\n",
            "15000/15000 [==============================] - 2s 113us/sample - loss: 0.0229 - acc: 0.9973 - val_loss: 0.3231 - val_acc: 0.8833\n",
            "Epoch 32/40\n",
            "15000/15000 [==============================] - 2s 111us/sample - loss: 0.0216 - acc: 0.9974 - val_loss: 0.3274 - val_acc: 0.8830\n",
            "Epoch 33/40\n",
            "15000/15000 [==============================] - 2s 114us/sample - loss: 0.0203 - acc: 0.9981 - val_loss: 0.3302 - val_acc: 0.8830\n",
            "Epoch 34/40\n",
            "15000/15000 [==============================] - 2s 113us/sample - loss: 0.0190 - acc: 0.9981 - val_loss: 0.3333 - val_acc: 0.8818\n",
            "Epoch 35/40\n",
            "15000/15000 [==============================] - 2s 111us/sample - loss: 0.0180 - acc: 0.9985 - val_loss: 0.3373 - val_acc: 0.8812\n",
            "Epoch 36/40\n",
            "15000/15000 [==============================] - 2s 111us/sample - loss: 0.0170 - acc: 0.9985 - val_loss: 0.3403 - val_acc: 0.8811\n",
            "Epoch 37/40\n",
            "15000/15000 [==============================] - 2s 114us/sample - loss: 0.0161 - acc: 0.9987 - val_loss: 0.3428 - val_acc: 0.8807\n",
            "Epoch 38/40\n",
            "15000/15000 [==============================] - 2s 113us/sample - loss: 0.0153 - acc: 0.9989 - val_loss: 0.3478 - val_acc: 0.8801\n",
            "Epoch 39/40\n",
            "15000/15000 [==============================] - 2s 115us/sample - loss: 0.0145 - acc: 0.9990 - val_loss: 0.3502 - val_acc: 0.8801\n",
            "Epoch 40/40\n",
            "15000/15000 [==============================] - 2s 115us/sample - loss: 0.0137 - acc: 0.9991 - val_loss: 0.3519 - val_acc: 0.8805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V6hV0fKYBqs0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "931bc5ed-7f2d-4a7b-c640-3ad980e577e0"
      },
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_datas, test_labels)\n",
        "print(results)"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 2s 72us/sample - loss: 0.3616 - acc: 0.8600\n",
            "[0.36161484493255613, 0.85996]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XdAQyHDbSGRs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "忧伤，终于看完了么？\n",
        "\n",
        "还没有，还有一个新技能：评估模型，分析模型表现如何。\n",
        "\n",
        "我们看上面的在测试集上面的准确率了么，它只有86-88%，但是我们在训练集上面已经到了99%的准确率了，我们需要找到上面这些\n",
        "```\n",
        "Epoch 13/40\n",
        "15000/15000 [==============================] - 2s 109us/sample - loss: 0.0751 - acc: 0.9863 - val_loss: 0.2831 - val_acc: 0.8877\n",
        "Epoch 14/40\n",
        "15000/15000 [==============================] - 2s 112us/sample - loss: 0.0700 - acc: 0.9875 - val_loss: 0.2822 - val_acc: 0.8876\n",
        "Epoch 15/40\n",
        "15000/15000 [==============================] - 2s 111us/sample - loss: 0.0652 - acc: 0.9882 - val_loss: 0.2833 - val_acc: 0.8872\n",
        "Epoch 16/40\n",
        "15000/15000 [==============================] - 2s 107us/sample - loss: 0.0606 - acc: 0.9893 - val_loss: 0.2854 - val_acc: 0.8879\n",
        "Epoch 17/40\n",
        "15000/15000 [==============================] - 2s 108us/sample - loss: 0.0564 - acc: 0.9907 - val_loss: 0.2867 - val_acc: 0.8869\n",
        "Epoch 18/40\n",
        "15000/15000 [==============================] - 2s 107us/sample - loss: 0.0527 - acc: 0.9919 - val_loss: 0.2885 - val_acc: 0.8883\n",
        "Epoch 19/40\n",
        "15000/15000 [==============================] - 2s 110us/sample - loss: 0.0491 - acc: 0.9925 - val_loss: 0.2918 - val_acc: 0.8859\n",
        "Epoch 20/40\n",
        "15000/15000 [==============================] - 2s 112us/sample - loss: 0.0461 - acc: 0.9929 - val_loss: 0.2934 - val_acc: 0.8865\n",
        "Epoch 21/40\n",
        "15000/15000 [==============================] - 2s 113us/sample - loss: 0.0430 - acc: 0.9934 - val_loss: 0.2954 - val_acc: 0.8867\n",
        "Epoch 22/40\n",
        "15000/15000 [==============================] - 2s 112us/sample - loss: 0.0405 - acc: 0.9939 - val_loss: 0.2991 - val_acc: 0.8859\n",
        "Epoch 23/40\n",
        "15000/15000 [==============================] - 2s 108us/sample - loss: 0.0378 - acc: 0.9949 - val_loss: 0.3007 - val_acc: 0.8861\n",
        "Epoch 24/40\n",
        "15000/15000 [==============================] - 2s 110us/sample - loss: 0.0353 - acc: 0.9951 - val_loss: 0.3034 - val_acc: 0.8863\n",
        "Epoch 25/40\n",
        "15000/15000 [==============================] - 2s 111us/sample - loss: 0.0331 - acc: 0.9955 - val_loss: 0.3056 - val_acc: 0.8864\n",
        "Epoch 26/40\n",
        "15000/15000 [==============================] - 2s 111us/sample - loss: 0.0310 - acc: 0.9956 - val_loss: 0.3085 - val_acc: 0.8851\n",
        "Epoch 27/40\n",
        "15000/15000 [==============================] - 2s 111us/sample - loss: 0.0291 - acc: 0.9961 - val_loss: 0.3121 - val_acc: 0.8852\n",
        "```\n",
        "\n",
        "到多久的时候在验证集上面就已经达到了稳定（拟合），我们需要在那个Epoch就停止训练，防止它出现过拟合（简单的来说就是训练集都对，测试集准确率确不高，缺少泛化性）\n",
        "\n",
        "通俗一点就是一味的看宇哥视频确没有动手刷数学题目，以至于遇到了不一样的题目就不会了（考研党的梗！！！）\n"
      ]
    },
    {
      "metadata": {
        "id": "nVLKw74cW-tZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Github"
      ]
    },
    {
      "metadata": {
        "id": "V3XSMzGxXFSr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CDih4gt_J0F_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Reference\n",
        "\n",
        "1. [Tensorflow官方教程](https://www.tensorflow.org/tutorials/keras/basic_text_classification) \n",
        "2. [pad_sequences函数](https://blog.csdn.net/HHTNAN/article/details/82585776)\n",
        "3. [什么是激活函数](https://www.zhihu.com/question/22334626)\n",
        "4. [测试集与验证集的区别](https://www.zhihu.com/question/26588665)\n",
        "5. [知乎中对词向量的解释](https://www.zhihu.com/question/21714667)\n",
        "6. [Embedding](https://blog.csdn.net/wangyangzhizhou/article/details/77530479)"
      ]
    },
    {
      "metadata": {
        "id": "erh4URrVsxzb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}