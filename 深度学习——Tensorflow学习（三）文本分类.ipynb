{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "深度学习——Tensorflow学习（三）文本分类.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JavanTang/Learn-a-little-tensorflow-every-day/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Tensorflow%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "j25r9NELJe_U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "简单概述一下本次的任务与昨天的任务类似都是做分类，但是我们需要使用tensorflow去做的是文本分类，将文本形式的影评分为正面与负面，我们使用的是的`数据集`是来自IMDB的数据集，之后我们还要下载一些中文的数据进行分析，在官方教程的基础上做扩充。\n",
        "\n",
        "\n",
        "## 任务简述\n",
        "1. 下载IMDB数据集\n",
        "2. 分析数据\n",
        "3. 将数据格式化\n",
        "4. 构建模型\n",
        "5. 训练模型\n",
        "6. 验证模型\n",
        "7. **评估模型**\n",
        "8. 使用中文数据集进行练习\n",
        "\n",
        "这里插一句本系列练习中都有Colaboratory，这个是Google大佬免费提供的机器学习的一个平台，如果有翻墙可以直接点Colaboratory链接，那样学习会更加轻松，排版也会较为舒服而且还可以随时修改参数。\n",
        "\n",
        "\n",
        "## 上代码"
      ]
    },
    {
      "metadata": {
        "id": "H2j26Tp96F9n",
        "colab_type": "code",
        "outputId": "4eff383c-fb61-4bf7-85ea-de3448c0677e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        }
      },
      "cell_type": "code",
      "source": [
        "# 导入包\n",
        "!pip install tensorflow==2.0.0-alpha0\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as k\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "print(tf.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.0.0-alpha0 in /usr/local/lib/python3.6/dist-packages (2.0.0a0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.33.1)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.0a20190301)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.1.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.11.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.16.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-alpha0) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (0.15.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-alpha0) (40.9.0)\n",
            "2.0.0-alpha0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OsltOR8W6Rf8",
        "colab_type": "code",
        "outputId": "be067728-dcbd-467b-d2ce-96a49ee93f2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "imdb = k.datasets.imdb\n",
        "\n",
        "(train_datas, train_labels), (test_datas, test_labels) = imdb.load_data()"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oYhsOEdi6wgy",
        "colab_type": "code",
        "outputId": "d1916399-57a1-4dcb-888f-9dc86edbadf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(train_datas[0])"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I3KC5S5e7G9R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "我们先查看第一个样本，这个都是数字组成的，那是因为IMDB将文字都化成了数字"
      ]
    },
    {
      "metadata": {
        "id": "92gYWYGy62YS",
        "colab_type": "code",
        "outputId": "ee688724-4270-4f96-8403-1b109c086bc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# 文字:数字的对应关系\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# 上面我们只有数字，所以我们现在需要将word_index变成“数字对应文字的关系”\n",
        "index_word = {v:k for k,v in word_index.items()}\n",
        "\n",
        "def decode_review(text):\n",
        "  '''\n",
        "  将text list中的number转换称为word\n",
        "  '''\n",
        "  return ' '.join([index_word.get(i, '?') for i in text])"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0smTQ2gx8gma",
        "colab_type": "code",
        "outputId": "a445245e-4f96-40fb-92cf-e50c191a62bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# 使用这个函数可以还原数据\n",
        "print(decode_review(train_datas[0]))"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room titillate it so heart shows to years of every never going villaronga help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but pratfalls to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other tricky in of seen over landed for anyone of gilmore's br show's to whether from than out themselves history he name half some br of 'n odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VJrWuEPIH18n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "下面开始构建模型，在构建模型之前我们回顾一下（二）中的图片分类有一个input_shape参数，当时这个参数我们选用了28*28，这是因为像素点矩阵是统一的，同时我们设置的模型内部的节点也是统一的（这句话不理解可以重新看看（一）中对神经网络的理解）\n",
        "\n",
        "这里为了长度标准化我们使用了pad_sequences函数。\n",
        "```\n",
        "函数说明： \n",
        "将长为nb_samples的序列（标量序列）转化为形如(nb_samples,nb_timesteps)2D numpy array。如果提供了参数maxlen，nb_timesteps=maxlen，否则其值为最长序列的长度。其他短于该长度的序列都会在后部填充0以达到该长度。长于nb_timesteps的序列将会被截断，以使其匹配目标长度。padding和截断发生的位置分别取决于padding和truncating. \n",
        "参数：\n",
        "sequences：浮点数或整数构成的两层嵌套列表\n",
        "maxlen：None或整数，为序列的最大长度。大于此长度的序列将被截短，小于此长度的序列将在后部填0.\n",
        "dtype：返回的numpy array的数据类型\n",
        "padding：‘pre’或‘post’，确定当需要补0时，在序列的起始还是结尾补\n",
        "truncating：‘pre’或‘post’，确定当需要截断序列时，从起始还是结尾截断\n",
        "value：浮点数，此值将在填充时代替默认的填充值0\n",
        "返回值： \n",
        "返回形如(nb_samples,nb_timesteps)的2D张量\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DYigHVusKSyY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_datas = k.preprocessing.sequence.pad_sequences(train_datas,\n",
        "                                                    maxlen=256,\n",
        "                                                    padding='post',\n",
        "                                                    value=0\n",
        "                                                        )\n",
        "test_datas = k.preprocessing.sequence.pad_sequences(test_datas,\n",
        "                                                   maxlen=256,\n",
        "                                                   padding='post',\n",
        "                                                   value=0\n",
        "                                                   )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rQB-7wRxMB1B",
        "colab_type": "code",
        "outputId": "26e26374-d86a-4f6a-cc2b-293a3a0685eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "cell_type": "code",
      "source": [
        "# 通过print我们可以发现后面的都被替换成了-1，同时所有的特征全部变成了256的长度\n",
        "print(train_datas[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[    1   194  1153   194  8255    78   228     5     6  1463  4369  5012\n",
            "   134    26     4   715     8   118  1634    14   394    20    13   119\n",
            "   954   189   102     5   207   110  3103    21    14    69   188     8\n",
            "    30    23     7     4   249   126    93     4   114     9  2300  1523\n",
            "     5   647     4   116     9    35  8163     4   229     9   340  1322\n",
            "     4   118     9     4   130  4901    19     4  1002     5    89    29\n",
            "   952    46    37     4   455     9    45    43    38  1543  1905   398\n",
            "     4  1649    26  6853     5   163    11  3215 10156     4  1153     9\n",
            "   194   775     7  8255 11596   349  2637   148   605 15358  8003    15\n",
            "   123   125    68 23141  6853    15   349   165  4362    98     5     4\n",
            "   228     9    43 36893  1157    15   299   120     5   120   174    11\n",
            "   220   175   136    50     9  4373   228  8255     5 25249   656   245\n",
            "  2350     5     4  9837   131   152   491    18 46151    32  7464  1212\n",
            "    14     9     6   371    78    22   625    64  1382     9     8   168\n",
            "   145    23     4  1690    15    16     4  1355     5    28     6    52\n",
            "   154   462    33    89    78   285    16   145    95     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dVEhl0UrMx78",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 构建模型, 这里我们先自行DIV一下\n",
        "# 先用（二）中图片分类的模型试试看\n",
        "\n",
        "model = k.Sequential([\n",
        "#     这个不用，具体原因可以看（一）中的解释\n",
        "#     keras.layers.Flatten(input_shape=(28,28)),\n",
        "    k.layers.Dense(128, activation=tf.nn.relu),\n",
        "    k.layers.Dense(2, activation=tf.nn.softmax)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WQxhukAWvQKh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ljyBt_qWvTQo",
        "colab_type": "code",
        "outputId": "47d5915a-5b45-4435-a092-2af381efe1aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1207
        }
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(train_datas,\n",
        "                    train_labels,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-821a6026c731>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     verbose=1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2501\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2502\u001b[0m         \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2503\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2504\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2505\u001b[0m       \u001b[0my_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[0;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[1;32m   2771\u001b[0m               self, '_contains_symbolic_tensors', False)\n\u001b[1;32m   2772\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expects_training_arg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2773\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2774\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2775\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m       \u001b[0;31m# `outputs` will be the inputs to the next layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    592\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m           \u001b[0;31m# Explicitly pass the learning phase placeholder to `call` if\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m           \u001b[0;31m# the `training` argument was left unspecified by the user.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1711\u001b[0m     \u001b[0;31m# Only call `build` if the user has manually overridden the build method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_is_default'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1713\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1714\u001b[0m     \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m     \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    961\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m       raise TypeError('Unable to build `Dense` layer with non-floating point '\n\u001b[0;32m--> 963\u001b[0;31m                       'dtype %s' % (dtype,))\n\u001b[0m\u001b[1;32m    964\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimension_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Unable to build `Dense` layer with non-floating point dtype <dtype: 'int32'>"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "vZm11ZsbsyWD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "准确性全部是50%，五五开？？？\n",
        "\n",
        "这里我们需要思考，为什么将一句话放进去出来的结果确是这样？在这个系列的学习中，主要是要学会思考，学会寻找问题并解决问题。\n",
        "\n",
        "我们来想想train_datas中的数据是什么样的，比如取第一条数据可能是:[\"223\".\"13\",\"3\",\"22\",\"19\",...,...]，其中这些数字是word_index中的序号，这些序号放入神经网络中为什么是五五开？\n",
        "\n",
        "给一下几个方向：\n",
        "\n",
        "1. 训练数据\n",
        "2. 设计的模型\n",
        "3. 损失函数\n",
        "\n",
        "训练数据：\n",
        "\n",
        "这个方面主要是我们数据的特征没有提取好，那我们如何解决这个问题？\n",
        "**Google 或者 Bing 或者 Baidu 搜索：神经网络 文本如何提取特征**，然后研读前5篇文章，这也是本次的练习，将五篇的文章总结一下，总结使用什么方法。\n",
        "\n",
        "在官方的教程中它直接直接使用了keras.layers.Embedding去提取了词向量。\n",
        "\n",
        "在那五篇文章中没有看到关于词向量的解释，可以看看这些[知乎中对词向量的解释](https://www.zhihu.com/question/21714667)，[Embedding](https://blog.csdn.net/wangyangzhizhou/article/details/77530479)\n",
        "\n",
        "那我们改写一下模型："
      ]
    },
    {
      "metadata": {
        "id": "K9mIMVcF8lIX",
        "colab_type": "code",
        "outputId": "20f26f02-a711-4e6b-82f0-135db27ce02c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "cell_type": "code",
      "source": [
        "train_datas[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    1,    14,    22,    16,    43,   530,   973,  1622,  1385,\n",
              "          65,   458,  4468,    66,  3941,     4,   173,    36,   256,\n",
              "           5,    25,   100,    43,   838,   112,    50,   670, 22665,\n",
              "           9,    35,   480,   284,     5,   150,     4,   172,   112,\n",
              "         167, 21631,   336,   385,    39,     4,   172,  4536,  1111,\n",
              "          17,   546,    38,    13,   447,     4,   192,    50,    16,\n",
              "           6,   147,  2025,    19,    14,    22,     4,  1920,  4613,\n",
              "         469,     4,    22,    71,    87,    12,    16,    43,   530,\n",
              "          38,    76,    15,    13,  1247,     4,    22,    17,   515,\n",
              "          17,    12,    16,   626,    18, 19193,     5,    62,   386,\n",
              "          12,     8,   316,     8,   106,     5,     4,  2223,  5244,\n",
              "          16,   480,    66,  3785,    33,     4,   130,    12,    16,\n",
              "          38,   619,     5,    25,   124,    51,    36,   135,    48,\n",
              "          25,  1415,    33,     6,    22,    12,   215,    28,    77,\n",
              "          52,     5,    14,   407,    16,    82, 10311,     8,     4,\n",
              "         107,   117,  5952,    15,   256,     4, 31050,     7,  3766,\n",
              "           5,   723,    36,    71,    43,   530,   476,    26,   400,\n",
              "         317,    46,     7,     4, 12118,  1029,    13,   104,    88,\n",
              "           4,   381,    15,   297,    98,    32,  2071,    56,    26,\n",
              "         141,     6,   194,  7486,    18,     4,   226,    22,    21,\n",
              "         134,   476,    26,   480,     5,   144,    30,  5535,    18,\n",
              "          51,    36,    28,   224,    92,    25,   104,     4,   226,\n",
              "          65,    16,    38,  1334,    88,    12,    16,   283,     5,\n",
              "          16,  4472,   113,   103,    32,    15,    16,  5345,    19,\n",
              "         178,    32,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "X5TE4bS-UQMe",
        "colab_type": "code",
        "outputId": "5730d9c5-5dd9-4f7e-88ca-83e0c88de378",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "cell_type": "code",
      "source": [
        "vob_len = len(index_word)+1\n",
        "\n",
        "print(vob_len)\n",
        "\n",
        "model = k.Sequential([\n",
        "    k.layers.Embedding(vob_len+2, 16),\n",
        "    k.layers.GlobalAveragePooling1D(),\n",
        "    k.layers.Dense(16, activation=tf.nn.relu),\n",
        "    k.layers.Dense(1, activation=tf.nn.sigmoid)\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 下面这条语句出现的的信息很重要，从这里我们一般就知道我们数据是怎么变化的，同时每一层出来的是什么我们也可以知道。\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "88585\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 16)          1417392   \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 1,417,681\n",
            "Trainable params: 1,417,681\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kSuJNN05JnUe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "在k.layers.Dense中有一个参数是activation，这里叫做激活函数[什么是激活函数](https://www.zhihu.com/question/22334626)\n",
        "\n",
        "这里又留了一个问题：\n",
        "\n",
        "**在keras中有几种激活函数，列举出来，当前我们已经使用过的这几种是什么意思？**\n",
        "\n",
        "\n",
        "\n",
        "上面那个代码大家还没有接触到的应该是`k.layers.GlobalAveragePooling1D`函数，留下几个问题：\n",
        "\n",
        "1. 这个函数的作用是大家的一个练习，自行去找资料去解决问题。\n",
        "2. 同时大家去掉这个来看看会出现什么问题。\n",
        "3. 对于loss与这个系列（一）不同，我这里简单的说一下就是binary_crossentropy，是处理二分类问题，具体的大家可以去网上自行查询，实在有困难可以留言评论。\n",
        "\n",
        "下面继续coding\n"
      ]
    },
    {
      "metadata": {
        "id": "4aCypuiJAW-S",
        "colab_type": "code",
        "outputId": "b265bd8e-288b-42cd-97c2-c2088492823d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1462
        }
      },
      "cell_type": "code",
      "source": [
        "#创建了验证集,前10000个做训练，后10000个做验证集\n",
        "#https://www.zhihu.com/question/26588665 这里解释了什么是测试集与验证集的\n",
        "x_val = train_datas[:10000]\n",
        "partial_x_train = train_datas[10000:]\n",
        "\n",
        "y_val = train_labels[:10000]\n",
        "partial_y_train = train_labels[10000:]\n",
        "\n",
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 15000 samples, validate on 10000 samples\n",
            "Epoch 1/40\n",
            "15000/15000 [==============================] - 2s 118us/sample - loss: 0.6923 - accuracy: 0.5051 - val_loss: 0.6911 - val_accuracy: 0.5012\n",
            "Epoch 2/40\n",
            "15000/15000 [==============================] - 1s 89us/sample - loss: 0.6881 - accuracy: 0.5931 - val_loss: 0.6851 - val_accuracy: 0.6223\n",
            "Epoch 3/40\n",
            "15000/15000 [==============================] - 1s 89us/sample - loss: 0.6779 - accuracy: 0.6801 - val_loss: 0.6722 - val_accuracy: 0.7102\n",
            "Epoch 4/40\n",
            "15000/15000 [==============================] - 1s 88us/sample - loss: 0.6585 - accuracy: 0.7223 - val_loss: 0.6501 - val_accuracy: 0.7552\n",
            "Epoch 5/40\n",
            "15000/15000 [==============================] - 1s 89us/sample - loss: 0.6279 - accuracy: 0.7770 - val_loss: 0.6186 - val_accuracy: 0.7756\n",
            "Epoch 6/40\n",
            "15000/15000 [==============================] - 1s 89us/sample - loss: 0.5867 - accuracy: 0.8155 - val_loss: 0.5799 - val_accuracy: 0.7926\n",
            "Epoch 7/40\n",
            "15000/15000 [==============================] - 1s 89us/sample - loss: 0.5383 - accuracy: 0.8322 - val_loss: 0.5360 - val_accuracy: 0.8176\n",
            "Epoch 8/40\n",
            "15000/15000 [==============================] - 1s 90us/sample - loss: 0.4877 - accuracy: 0.8561 - val_loss: 0.4942 - val_accuracy: 0.8312\n",
            "Epoch 9/40\n",
            "15000/15000 [==============================] - 1s 89us/sample - loss: 0.4389 - accuracy: 0.8717 - val_loss: 0.4554 - val_accuracy: 0.8416\n",
            "Epoch 10/40\n",
            "15000/15000 [==============================] - 1s 91us/sample - loss: 0.3941 - accuracy: 0.8874 - val_loss: 0.4222 - val_accuracy: 0.8495\n",
            "Epoch 11/40\n",
            "15000/15000 [==============================] - 1s 92us/sample - loss: 0.3548 - accuracy: 0.8974 - val_loss: 0.3942 - val_accuracy: 0.8595\n",
            "Epoch 12/40\n",
            "15000/15000 [==============================] - 1s 92us/sample - loss: 0.3210 - accuracy: 0.9063 - val_loss: 0.3725 - val_accuracy: 0.8632\n",
            "Epoch 13/40\n",
            "15000/15000 [==============================] - 1s 92us/sample - loss: 0.2925 - accuracy: 0.9140 - val_loss: 0.3529 - val_accuracy: 0.8699\n",
            "Epoch 14/40\n",
            "15000/15000 [==============================] - 1s 92us/sample - loss: 0.2670 - accuracy: 0.9211 - val_loss: 0.3385 - val_accuracy: 0.8736\n",
            "Epoch 15/40\n",
            "15000/15000 [==============================] - 1s 91us/sample - loss: 0.2454 - accuracy: 0.9282 - val_loss: 0.3263 - val_accuracy: 0.8770\n",
            "Epoch 16/40\n",
            "15000/15000 [==============================] - 1s 92us/sample - loss: 0.2261 - accuracy: 0.9331 - val_loss: 0.3163 - val_accuracy: 0.8771\n",
            "Epoch 17/40\n",
            "15000/15000 [==============================] - 1s 93us/sample - loss: 0.2086 - accuracy: 0.9399 - val_loss: 0.3082 - val_accuracy: 0.8799\n",
            "Epoch 18/40\n",
            "15000/15000 [==============================] - 1s 93us/sample - loss: 0.1930 - accuracy: 0.9449 - val_loss: 0.3008 - val_accuracy: 0.8824\n",
            "Epoch 19/40\n",
            "15000/15000 [==============================] - 1s 96us/sample - loss: 0.1789 - accuracy: 0.9490 - val_loss: 0.2951 - val_accuracy: 0.8840\n",
            "Epoch 20/40\n",
            "15000/15000 [==============================] - 1s 95us/sample - loss: 0.1665 - accuracy: 0.9533 - val_loss: 0.2913 - val_accuracy: 0.8851\n",
            "Epoch 21/40\n",
            "15000/15000 [==============================] - 1s 95us/sample - loss: 0.1543 - accuracy: 0.9581 - val_loss: 0.2874 - val_accuracy: 0.8845\n",
            "Epoch 22/40\n",
            "15000/15000 [==============================] - 1s 93us/sample - loss: 0.1439 - accuracy: 0.9627 - val_loss: 0.2838 - val_accuracy: 0.8878\n",
            "Epoch 23/40\n",
            "15000/15000 [==============================] - 1s 92us/sample - loss: 0.1337 - accuracy: 0.9660 - val_loss: 0.2826 - val_accuracy: 0.8877\n",
            "Epoch 24/40\n",
            "15000/15000 [==============================] - 1s 92us/sample - loss: 0.1249 - accuracy: 0.9689 - val_loss: 0.2808 - val_accuracy: 0.8881\n",
            "Epoch 25/40\n",
            "15000/15000 [==============================] - 1s 94us/sample - loss: 0.1164 - accuracy: 0.9724 - val_loss: 0.2782 - val_accuracy: 0.8878\n",
            "Epoch 26/40\n",
            "15000/15000 [==============================] - 1s 93us/sample - loss: 0.1087 - accuracy: 0.9756 - val_loss: 0.2789 - val_accuracy: 0.8875\n",
            "Epoch 27/40\n",
            "15000/15000 [==============================] - 1s 95us/sample - loss: 0.1016 - accuracy: 0.9779 - val_loss: 0.2774 - val_accuracy: 0.8891\n",
            "Epoch 28/40\n",
            "15000/15000 [==============================] - 1s 95us/sample - loss: 0.0949 - accuracy: 0.9798 - val_loss: 0.2771 - val_accuracy: 0.8900\n",
            "Epoch 29/40\n",
            "15000/15000 [==============================] - 1s 92us/sample - loss: 0.0890 - accuracy: 0.9811 - val_loss: 0.2787 - val_accuracy: 0.8879\n",
            "Epoch 30/40\n",
            "15000/15000 [==============================] - 1s 92us/sample - loss: 0.0837 - accuracy: 0.9829 - val_loss: 0.2777 - val_accuracy: 0.8904\n",
            "Epoch 31/40\n",
            "15000/15000 [==============================] - 1s 92us/sample - loss: 0.0779 - accuracy: 0.9848 - val_loss: 0.2781 - val_accuracy: 0.8903\n",
            "Epoch 32/40\n",
            "15000/15000 [==============================] - 1s 92us/sample - loss: 0.0731 - accuracy: 0.9862 - val_loss: 0.2789 - val_accuracy: 0.8898\n",
            "Epoch 33/40\n",
            "15000/15000 [==============================] - 1s 92us/sample - loss: 0.0685 - accuracy: 0.9870 - val_loss: 0.2805 - val_accuracy: 0.8902\n",
            "Epoch 34/40\n",
            "15000/15000 [==============================] - 1s 92us/sample - loss: 0.0644 - accuracy: 0.9885 - val_loss: 0.2820 - val_accuracy: 0.8901\n",
            "Epoch 35/40\n",
            "15000/15000 [==============================] - 1s 92us/sample - loss: 0.0610 - accuracy: 0.9891 - val_loss: 0.2834 - val_accuracy: 0.8894\n",
            "Epoch 36/40\n",
            "15000/15000 [==============================] - 1s 94us/sample - loss: 0.0570 - accuracy: 0.9904 - val_loss: 0.2849 - val_accuracy: 0.8895\n",
            "Epoch 37/40\n",
            "15000/15000 [==============================] - 1s 94us/sample - loss: 0.0535 - accuracy: 0.9909 - val_loss: 0.2869 - val_accuracy: 0.8898\n",
            "Epoch 38/40\n",
            "15000/15000 [==============================] - 1s 93us/sample - loss: 0.0505 - accuracy: 0.9921 - val_loss: 0.2902 - val_accuracy: 0.8881\n",
            "Epoch 39/40\n",
            "15000/15000 [==============================] - 1s 93us/sample - loss: 0.0481 - accuracy: 0.9923 - val_loss: 0.2918 - val_accuracy: 0.8889\n",
            "Epoch 40/40\n",
            "15000/15000 [==============================] - 1s 93us/sample - loss: 0.0449 - accuracy: 0.9934 - val_loss: 0.2933 - val_accuracy: 0.8877\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "V6hV0fKYBqs0",
        "colab_type": "code",
        "outputId": "1803d026-b0f5-4d7c-cdfd-886cdd9fb336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_datas, test_labels)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 1s 40us/sample - loss: 0.3240 - accuracy: 0.8728\n",
            "[0.3240479930019379, 0.87284]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-Y2_IWokMk_k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "忧伤，终于看完了么？\n",
        "\n",
        "还没有，还有一个新技能：评估模型，分析模型表现如何。\n",
        "\n",
        "我们看上面的在测试集上面的准确率了么，它只有86-88%，但是我们在训练集上面已经到了99%的准确率了，我们需要找到上面这些\n",
        "```\n",
        "Epoch 13/40\n",
        "15000/15000 [==============================] - 2s 109us/sample - loss: 0.0751 - acc: 0.9863 - val_loss: 0.2831 - val_acc: 0.8877\n",
        "Epoch 14/40\n",
        "15000/15000 [==============================] - 2s 112us/sample - loss: 0.0700 - acc: 0.9875 - val_loss: 0.2822 - val_acc: 0.8876\n",
        "Epoch 15/40\n",
        "15000/15000 [==============================] - 2s 111us/sample - loss: 0.0652 - acc: 0.9882 - val_loss: 0.2833 - val_acc: 0.8872\n",
        "Epoch 16/40\n",
        "15000/15000 [==============================] - 2s 107us/sample - loss: 0.0606 - acc: 0.9893 - val_loss: 0.2854 - val_acc: 0.8879\n",
        "Epoch 17/40\n",
        "15000/15000 [==============================] - 2s 108us/sample - loss: 0.0564 - acc: 0.9907 - val_loss: 0.2867 - val_acc: 0.8869\n",
        "Epoch 18/40\n",
        "15000/15000 [==============================] - 2s 107us/sample - loss: 0.0527 - acc: 0.9919 - val_loss: 0.2885 - val_acc: 0.8883\n",
        "Epoch 19/40\n",
        "15000/15000 [==============================] - 2s 110us/sample - loss: 0.0491 - acc: 0.9925 - val_loss: 0.2918 - val_acc: 0.8859\n",
        "Epoch 20/40\n",
        "15000/15000 [==============================] - 2s 112us/sample - loss: 0.0461 - acc: 0.9929 - val_loss: 0.2934 - val_acc: 0.8865\n",
        "Epoch 21/40\n",
        "15000/15000 [==============================] - 2s 113us/sample - loss: 0.0430 - acc: 0.9934 - val_loss: 0.2954 - val_acc: 0.8867\n",
        "Epoch 22/40\n",
        "15000/15000 [==============================] - 2s 112us/sample - loss: 0.0405 - acc: 0.9939 - val_loss: 0.2991 - val_acc: 0.8859\n",
        "Epoch 23/40\n",
        "15000/15000 [==============================] - 2s 108us/sample - loss: 0.0378 - acc: 0.9949 - val_loss: 0.3007 - val_acc: 0.8861\n",
        "Epoch 24/40\n",
        "15000/15000 [==============================] - 2s 110us/sample - loss: 0.0353 - acc: 0.9951 - val_loss: 0.3034 - val_acc: 0.8863\n",
        "Epoch 25/40\n",
        "15000/15000 [==============================] - 2s 111us/sample - loss: 0.0331 - acc: 0.9955 - val_loss: 0.3056 - val_acc: 0.8864\n",
        "Epoch 26/40\n",
        "15000/15000 [==============================] - 2s 111us/sample - loss: 0.0310 - acc: 0.9956 - val_loss: 0.3085 - val_acc: 0.8851\n",
        "Epoch 27/40\n",
        "15000/15000 [==============================] - 2s 111us/sample - loss: 0.0291 - acc: 0.9961 - val_loss: 0.3121 - val_acc: 0.8852\n",
        "```\n",
        "\n",
        "到多久的时候在验证集上面就已经达到了稳定（拟合），我们需要在那个Epoch就停止训练，防止它出现过拟合（简单的来说就是训练集都对，测试集准确率确不高，缺少泛化性）\n",
        "\n",
        "通俗一点就是一味的看宇哥视频确没有动手刷数学题目，以至于遇到了不一样的题目就不会了（考研党的梗！！！）"
      ]
    },
    {
      "metadata": {
        "id": "Pl93VuWsXmr8",
        "colab_type": "code",
        "outputId": "05341728-51f5-4ff0-a6ff-133d138fa470",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "cell_type": "code",
      "source": [
        "# 这个命令可能失效\n",
        "# Google网盘：https://drive.google.com/open?id=1OXWKZwfTpAXpuyCb4ESZq_UrycN9YvHe\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1OXWKZwfTpAXpuyCb4ESZq_UrycN9YvHe' -O dmsc.csv\n",
        "\n",
        "# 我直接是在colab中Coding，所以我直接使用网盘当我的文件夹。 这个操作可以看参考7.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# 执行这个可以直接看到网盘的内容 \n",
        "!ls \"/content/drive/My Drive/\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-09 01:16:38--  https://docs.google.com/uc?export=download&id=1OXWKZwfTpAXpuyCb4ESZq_UrycN9YvHe\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.142.101, 74.125.142.138, 74.125.142.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.142.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘dmsc.csv’\n",
            "\n",
            "\rdmsc.csv                [<=>                 ]       0  --.-KB/s               \rdmsc.csv                [ <=>                ]   3.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-04-09 01:16:38 (31.0 MB/s) - ‘dmsc.csv’ saved [3244]\n",
            "\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n",
            " 6139-supervised-word-movers-distance.pdf\n",
            " Algorithm\n",
            " app\n",
            "'Colab Notebooks'\n",
            " dataset\n",
            " distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
            " InSea-master.zip\n",
            " 无标题演示文稿.gslides\n",
            " 无标题绘图.gdraw\n",
            " 未命名网站.gsite\n",
            " 电子行程单2019-03-11.pdf\n",
            " 翰正资料\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "beoTeXeOLQuN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dmsc = pd.read_csv('drive/My Drive/dataset/DMSC.csv',encoding = \"utf-8\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4v86gdyFQMNA",
        "colab_type": "code",
        "outputId": "5d2a8ac3-2283-40c1-a041-af9d7dabf50e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# 使用分词工具1\n",
        "!pip install pkuseg\n",
        "import re\n",
        "import pkuseg"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pkuseg in /usr/local/lib/python3.6/dist-packages (0.0.21)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from pkuseg) (1.16.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kpe7qJK2pp1Q",
        "colab_type": "code",
        "outputId": "e8e0688e-c5fb-4413-af08-491f98610dd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# 设置正负样本各自的数量\n",
        "size = 10000\n",
        "\n",
        "from multiprocessing import Pool\n",
        "\n",
        "p = Pool()\n",
        "\n",
        "dmsc_dataset = dmsc[dmsc['Star']!=3].loc[:,['Star','Comment']]\n",
        "xl_data_label = dmsc_dataset['Star'].values\n",
        "xl_data_review = dmsc_dataset['Comment'].values\n",
        "\n",
        "label = []\n",
        "review = []\n",
        "\n",
        "word_index = {}\n",
        "# 将空格的编号赋值为0,未知符号设置为1\n",
        "word_index[' '] = 0\n",
        "word_index['<unknow>'] = 1\n",
        "\n",
        "seg = pkuseg.pkuseg('web')\n",
        "\n",
        "print('开始进行分词，总数%d' % len(xl_data_review))\n",
        "\n",
        "def seg_cut(i):\n",
        "  text = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[。+、——！.…，。？、~@#￥%……&*（）]+\", \" \",xl_data_review[i])\n",
        "  _build = seg.cut(text)\n",
        "  result = []\n",
        "  for j in _build:\n",
        "    # 如果这个词没有出现过，则将它的编号往后+1\n",
        "    if not j in word_index:\n",
        "      word_index[j] = len(word_index) + 1\n",
        "    result.append(word_index[j]) # 这里就已经将文字转换数值的数据保存下来了\n",
        "  result = np.array(result)\n",
        "  review.append(result) # 将所有文本的数据，变成数值型\n",
        "  label.append(xl_data_label[i])\n",
        "  \n",
        "post = 0\n",
        "neg = 0\n",
        "# 简单处理label，将它变为 0 1 二分类问题\n",
        "for i in range(len(xl_data_label)):\n",
        "  if xl_data_label[i] > 3:\n",
        "    xl_data_label[i] = 1\n",
        "  elif xl_data_label[i] < 3:\n",
        "    xl_data_label[i] = 0\n",
        "\n",
        "# 收集所有的词,将每个词对应一个编号\n",
        "for i in range(len(xl_data_review)):\n",
        "#   p.apply_async(seg_cut, args=(i,))\n",
        "  if xl_data_label[i] == 1 and post < size:\n",
        "    post += 1\n",
        "    seg_cut(i)\n",
        "  if xl_data_label[i] == 0 and neg <size:\n",
        "    neg += 1\n",
        "    seg_cut(i)\n",
        "  if post >= size and neg >= size:\n",
        "    break\n",
        "  if (post+neg) % 10000 == 0:\n",
        "    print('已经加载成功%d篇' % (post+neg))\n",
        "p.close()\n",
        "p.join()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "开始进行分词，总数1650497\n",
            "已经加载成功10000篇\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xwm-20Pk5t9F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "下面我们可以看看我们现在训练数据全部变成了字符型了"
      ]
    },
    {
      "metadata": {
        "id": "rrnwPUaMLYtH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "label = np.array(label)\n",
        "train_x, test_x, train_y, test_y = train_test_split(review, label, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5JkR1mx5QXSv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e61040e-86a1-4a52-9f20-d28c25ef12ad"
      },
      "cell_type": "code",
      "source": [
        "train_y.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13400,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "-Dq59xX-raTx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_x = k.preprocessing.sequence.pad_sequences(train_x,\n",
        "                                                    maxlen=256,\n",
        "                                                    padding='post',\n",
        "                                                    value=0\n",
        "                                                        )\n",
        "test_x = k.preprocessing.sequence.pad_sequences(test_x,\n",
        "                                                   maxlen=256,\n",
        "                                                   padding='post',\n",
        "                                                   value=0\n",
        "                                                   )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7L7jB41T6aGA",
        "colab_type": "code",
        "outputId": "1d5bc39d-0691-4185-a892-f679442827a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# 我们这里再写一个函数还原它\n",
        "index_word = {v:k for k,v in word_index.items()}\n",
        "def reduction_number(data):\n",
        "  '''\n",
        "  data list => 文字\n",
        "  '''\n",
        "  return ''.join([index_word[i] for i in data])\n",
        "\n",
        "print(reduction_number(train_x[0]))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "因为1对她有太多期待所以导致这部续作不尽如人意                                                                                                                                                                                                                                                  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KKedhdYG9tGK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "终于处理成为和之前一样的效果了，那我们开始和之前一样的操作\n",
        "\n",
        "1. 构建模型\n",
        "2. 设置模型的一些优化参数\n",
        "3. 训练模型\n",
        "4. 验证模型"
      ]
    },
    {
      "metadata": {
        "id": "9vY22mPL8CFF",
        "colab_type": "code",
        "outputId": "90cab236-3d58-484c-c733-cbeb5675168b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "cell_type": "code",
      "source": [
        "# 构建模型\n",
        "vob_len = len(word_index) + 1\n",
        "dmsc_model = k.Sequential([\n",
        "    k.layers.Embedding(vob_len, 16),\n",
        "    k.layers.GlobalAveragePooling1D(),\n",
        "    k.layers.Dense(16, activation=tf.nn.relu),\n",
        "    k.layers.Dense(1, activation=tf.nn.sigmoid)\n",
        "\n",
        "])\n",
        "dmsc_model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 16)          487312    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_1 ( (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 487,601\n",
            "Trainable params: 487,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oU3apjw69t30",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 设置模型优化参数\n",
        "\n",
        "dmsc_model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f4TFjBkk_quc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d73ed665-f55a-43bf-d321-46a91893960e"
      },
      "cell_type": "code",
      "source": [
        "train_x.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13400, 256)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "79yY3Y-Z9oPt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3574
        },
        "outputId": "8867c0e9-f7d2-4c2b-b916-f413a6ce6365"
      },
      "cell_type": "code",
      "source": [
        "# 训练模型\n",
        "\n",
        "x_val = train_x[:1000]\n",
        "y_val = train_y[:1000]\n",
        "partial_x_train = train_x[1000:]\n",
        "partial_y_train = train_y[1000:]\n",
        "\n",
        "dmsc_history = dmsc_model.fit(partial_x_train,\n",
        "                        partial_y_train,\n",
        "                        epochs=100,\n",
        "                        batch_size=512,\n",
        "                        validation_data=(x_val, y_val),\n",
        "                        verbose=1)\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 12400 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "12400/12400 [==============================] - 1s 77us/sample - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6929 - val_accuracy: 0.4850\n",
            "Epoch 2/100\n",
            "12400/12400 [==============================] - 1s 60us/sample - loss: 0.6926 - accuracy: 0.5262 - val_loss: 0.6923 - val_accuracy: 0.7220\n",
            "Epoch 3/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.6920 - accuracy: 0.5326 - val_loss: 0.6918 - val_accuracy: 0.5140\n",
            "Epoch 4/100\n",
            "12400/12400 [==============================] - 1s 58us/sample - loss: 0.6912 - accuracy: 0.6485 - val_loss: 0.6909 - val_accuracy: 0.5400\n",
            "Epoch 5/100\n",
            "12400/12400 [==============================] - 1s 58us/sample - loss: 0.6900 - accuracy: 0.5910 - val_loss: 0.6892 - val_accuracy: 0.6200\n",
            "Epoch 6/100\n",
            "12400/12400 [==============================] - 1s 60us/sample - loss: 0.6883 - accuracy: 0.5934 - val_loss: 0.6872 - val_accuracy: 0.7600\n",
            "Epoch 7/100\n",
            "12400/12400 [==============================] - 1s 60us/sample - loss: 0.6856 - accuracy: 0.6944 - val_loss: 0.6845 - val_accuracy: 0.6990\n",
            "Epoch 8/100\n",
            "12400/12400 [==============================] - 1s 60us/sample - loss: 0.6817 - accuracy: 0.7382 - val_loss: 0.6800 - val_accuracy: 0.7970\n",
            "Epoch 9/100\n",
            "12400/12400 [==============================] - 1s 60us/sample - loss: 0.6764 - accuracy: 0.7967 - val_loss: 0.6745 - val_accuracy: 0.7980\n",
            "Epoch 10/100\n",
            "12400/12400 [==============================] - 1s 60us/sample - loss: 0.6696 - accuracy: 0.7846 - val_loss: 0.6681 - val_accuracy: 0.7550\n",
            "Epoch 11/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.6610 - accuracy: 0.7707 - val_loss: 0.6586 - val_accuracy: 0.7400\n",
            "Epoch 12/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.6509 - accuracy: 0.7783 - val_loss: 0.6505 - val_accuracy: 0.7440\n",
            "Epoch 13/100\n",
            "12400/12400 [==============================] - 1s 58us/sample - loss: 0.6387 - accuracy: 0.8177 - val_loss: 0.6385 - val_accuracy: 0.7720\n",
            "Epoch 14/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.6255 - accuracy: 0.8086 - val_loss: 0.6263 - val_accuracy: 0.7750\n",
            "Epoch 15/100\n",
            "12400/12400 [==============================] - 1s 58us/sample - loss: 0.6109 - accuracy: 0.8178 - val_loss: 0.6133 - val_accuracy: 0.7740\n",
            "Epoch 16/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.5964 - accuracy: 0.7856 - val_loss: 0.6027 - val_accuracy: 0.7400\n",
            "Epoch 17/100\n",
            "12400/12400 [==============================] - 1s 58us/sample - loss: 0.5805 - accuracy: 0.8095 - val_loss: 0.5889 - val_accuracy: 0.7630\n",
            "Epoch 18/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.5650 - accuracy: 0.8244 - val_loss: 0.5729 - val_accuracy: 0.8090\n",
            "Epoch 19/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.5494 - accuracy: 0.8191 - val_loss: 0.5637 - val_accuracy: 0.7670\n",
            "Epoch 20/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.5337 - accuracy: 0.8409 - val_loss: 0.5493 - val_accuracy: 0.7820\n",
            "Epoch 21/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.5191 - accuracy: 0.8423 - val_loss: 0.5360 - val_accuracy: 0.8040\n",
            "Epoch 22/100\n",
            "12400/12400 [==============================] - 1s 60us/sample - loss: 0.5049 - accuracy: 0.8459 - val_loss: 0.5270 - val_accuracy: 0.7830\n",
            "Epoch 23/100\n",
            "12400/12400 [==============================] - 1s 60us/sample - loss: 0.4902 - accuracy: 0.8538 - val_loss: 0.5141 - val_accuracy: 0.8070\n",
            "Epoch 24/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.4764 - accuracy: 0.8601 - val_loss: 0.5034 - val_accuracy: 0.8090\n",
            "Epoch 25/100\n",
            "12400/12400 [==============================] - 1s 58us/sample - loss: 0.4626 - accuracy: 0.8621 - val_loss: 0.4948 - val_accuracy: 0.7970\n",
            "Epoch 26/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.4495 - accuracy: 0.8648 - val_loss: 0.4897 - val_accuracy: 0.7760\n",
            "Epoch 27/100\n",
            "12400/12400 [==============================] - 1s 58us/sample - loss: 0.4374 - accuracy: 0.8642 - val_loss: 0.4763 - val_accuracy: 0.8140\n",
            "Epoch 28/100\n",
            "12400/12400 [==============================] - 1s 58us/sample - loss: 0.4263 - accuracy: 0.8648 - val_loss: 0.4684 - val_accuracy: 0.8120\n",
            "Epoch 29/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.4144 - accuracy: 0.8735 - val_loss: 0.4622 - val_accuracy: 0.8100\n",
            "Epoch 30/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.4045 - accuracy: 0.8749 - val_loss: 0.4587 - val_accuracy: 0.7930\n",
            "Epoch 31/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.3940 - accuracy: 0.8797 - val_loss: 0.4488 - val_accuracy: 0.8200\n",
            "Epoch 32/100\n",
            "12400/12400 [==============================] - 1s 63us/sample - loss: 0.3850 - accuracy: 0.8813 - val_loss: 0.4434 - val_accuracy: 0.8200\n",
            "Epoch 33/100\n",
            "12400/12400 [==============================] - 1s 62us/sample - loss: 0.3758 - accuracy: 0.8825 - val_loss: 0.4383 - val_accuracy: 0.8200\n",
            "Epoch 34/100\n",
            "12400/12400 [==============================] - 1s 61us/sample - loss: 0.3670 - accuracy: 0.8869 - val_loss: 0.4360 - val_accuracy: 0.8060\n",
            "Epoch 35/100\n",
            "12400/12400 [==============================] - 1s 62us/sample - loss: 0.3592 - accuracy: 0.8875 - val_loss: 0.4296 - val_accuracy: 0.8170\n",
            "Epoch 36/100\n",
            "12400/12400 [==============================] - 1s 62us/sample - loss: 0.3520 - accuracy: 0.8893 - val_loss: 0.4261 - val_accuracy: 0.8210\n",
            "Epoch 37/100\n",
            "12400/12400 [==============================] - 1s 63us/sample - loss: 0.3440 - accuracy: 0.8912 - val_loss: 0.4314 - val_accuracy: 0.7940\n",
            "Epoch 38/100\n",
            "12400/12400 [==============================] - 1s 62us/sample - loss: 0.3370 - accuracy: 0.8941 - val_loss: 0.4203 - val_accuracy: 0.8170\n",
            "Epoch 39/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.3293 - accuracy: 0.8968 - val_loss: 0.4150 - val_accuracy: 0.8250\n",
            "Epoch 40/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.3231 - accuracy: 0.8967 - val_loss: 0.4122 - val_accuracy: 0.8210\n",
            "Epoch 41/100\n",
            "12400/12400 [==============================] - 1s 60us/sample - loss: 0.3166 - accuracy: 0.8978 - val_loss: 0.4106 - val_accuracy: 0.8180\n",
            "Epoch 42/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.3088 - accuracy: 0.9026 - val_loss: 0.4091 - val_accuracy: 0.8170\n",
            "Epoch 43/100\n",
            "12400/12400 [==============================] - 1s 58us/sample - loss: 0.3027 - accuracy: 0.9050 - val_loss: 0.4042 - val_accuracy: 0.8290\n",
            "Epoch 44/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.2971 - accuracy: 0.9057 - val_loss: 0.4022 - val_accuracy: 0.8250\n",
            "Epoch 45/100\n",
            "12400/12400 [==============================] - 1s 60us/sample - loss: 0.2912 - accuracy: 0.9068 - val_loss: 0.4085 - val_accuracy: 0.8010\n",
            "Epoch 46/100\n",
            "12400/12400 [==============================] - 1s 61us/sample - loss: 0.2859 - accuracy: 0.9085 - val_loss: 0.4027 - val_accuracy: 0.8170\n",
            "Epoch 47/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.2806 - accuracy: 0.9096 - val_loss: 0.4021 - val_accuracy: 0.8150\n",
            "Epoch 48/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.2752 - accuracy: 0.9135 - val_loss: 0.4007 - val_accuracy: 0.8130\n",
            "Epoch 49/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.2704 - accuracy: 0.9131 - val_loss: 0.3978 - val_accuracy: 0.8160\n",
            "Epoch 50/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.2665 - accuracy: 0.9130 - val_loss: 0.3956 - val_accuracy: 0.8200\n",
            "Epoch 51/100\n",
            "12400/12400 [==============================] - 1s 58us/sample - loss: 0.2617 - accuracy: 0.9147 - val_loss: 0.3987 - val_accuracy: 0.8090\n",
            "Epoch 52/100\n",
            "12400/12400 [==============================] - 1s 58us/sample - loss: 0.2568 - accuracy: 0.9177 - val_loss: 0.3915 - val_accuracy: 0.8260\n",
            "Epoch 53/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.2523 - accuracy: 0.9198 - val_loss: 0.3907 - val_accuracy: 0.8270\n",
            "Epoch 54/100\n",
            "12400/12400 [==============================] - 1s 60us/sample - loss: 0.2492 - accuracy: 0.9197 - val_loss: 0.3904 - val_accuracy: 0.8220\n",
            "Epoch 55/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.2442 - accuracy: 0.9228 - val_loss: 0.3895 - val_accuracy: 0.8240\n",
            "Epoch 56/100\n",
            "12400/12400 [==============================] - 1s 58us/sample - loss: 0.2402 - accuracy: 0.9231 - val_loss: 0.3887 - val_accuracy: 0.8270\n",
            "Epoch 57/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.2372 - accuracy: 0.9248 - val_loss: 0.3940 - val_accuracy: 0.8150\n",
            "Epoch 58/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.2330 - accuracy: 0.9257 - val_loss: 0.3934 - val_accuracy: 0.8140\n",
            "Epoch 59/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.2292 - accuracy: 0.9258 - val_loss: 0.3871 - val_accuracy: 0.8190\n",
            "Epoch 60/100\n",
            "12400/12400 [==============================] - 1s 60us/sample - loss: 0.2271 - accuracy: 0.9266 - val_loss: 0.3883 - val_accuracy: 0.8180\n",
            "Epoch 61/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.2224 - accuracy: 0.9294 - val_loss: 0.3913 - val_accuracy: 0.8150\n",
            "Epoch 62/100\n",
            "12400/12400 [==============================] - 1s 58us/sample - loss: 0.2192 - accuracy: 0.9310 - val_loss: 0.3901 - val_accuracy: 0.8170\n",
            "Epoch 63/100\n",
            "12400/12400 [==============================] - 1s 60us/sample - loss: 0.2158 - accuracy: 0.9301 - val_loss: 0.3889 - val_accuracy: 0.8170\n",
            "Epoch 64/100\n",
            "12400/12400 [==============================] - 1s 60us/sample - loss: 0.2135 - accuracy: 0.9319 - val_loss: 0.3873 - val_accuracy: 0.8210\n",
            "Epoch 65/100\n",
            "12400/12400 [==============================] - 1s 58us/sample - loss: 0.2107 - accuracy: 0.9327 - val_loss: 0.3873 - val_accuracy: 0.8230\n",
            "Epoch 66/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.2075 - accuracy: 0.9340 - val_loss: 0.3872 - val_accuracy: 0.8200\n",
            "Epoch 67/100\n",
            "12400/12400 [==============================] - 1s 60us/sample - loss: 0.2039 - accuracy: 0.9367 - val_loss: 0.3878 - val_accuracy: 0.8140\n",
            "Epoch 68/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.2015 - accuracy: 0.9357 - val_loss: 0.3937 - val_accuracy: 0.8190\n",
            "Epoch 69/100\n",
            "12400/12400 [==============================] - 1s 60us/sample - loss: 0.1997 - accuracy: 0.9362 - val_loss: 0.3893 - val_accuracy: 0.8130\n",
            "Epoch 70/100\n",
            "12400/12400 [==============================] - 1s 60us/sample - loss: 0.1964 - accuracy: 0.9367 - val_loss: 0.3882 - val_accuracy: 0.8190\n",
            "Epoch 71/100\n",
            "12400/12400 [==============================] - 1s 58us/sample - loss: 0.1936 - accuracy: 0.9388 - val_loss: 0.3905 - val_accuracy: 0.8120\n",
            "Epoch 72/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.1907 - accuracy: 0.9390 - val_loss: 0.3905 - val_accuracy: 0.8140\n",
            "Epoch 73/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.1883 - accuracy: 0.9402 - val_loss: 0.3928 - val_accuracy: 0.8120\n",
            "Epoch 74/100\n",
            "12400/12400 [==============================] - 1s 58us/sample - loss: 0.1870 - accuracy: 0.9402 - val_loss: 0.3978 - val_accuracy: 0.8190\n",
            "Epoch 75/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.1844 - accuracy: 0.9419 - val_loss: 0.3952 - val_accuracy: 0.8180\n",
            "Epoch 76/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.1812 - accuracy: 0.9418 - val_loss: 0.3916 - val_accuracy: 0.8210\n",
            "Epoch 77/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.1799 - accuracy: 0.9410 - val_loss: 0.3927 - val_accuracy: 0.8200\n",
            "Epoch 78/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.1777 - accuracy: 0.9422 - val_loss: 0.3936 - val_accuracy: 0.8130\n",
            "Epoch 79/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.1750 - accuracy: 0.9431 - val_loss: 0.3938 - val_accuracy: 0.8190\n",
            "Epoch 80/100\n",
            "12400/12400 [==============================] - 1s 58us/sample - loss: 0.1729 - accuracy: 0.9448 - val_loss: 0.3951 - val_accuracy: 0.8160\n",
            "Epoch 81/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.1707 - accuracy: 0.9464 - val_loss: 0.3973 - val_accuracy: 0.8130\n",
            "Epoch 82/100\n",
            "12400/12400 [==============================] - 1s 58us/sample - loss: 0.1687 - accuracy: 0.9456 - val_loss: 0.3988 - val_accuracy: 0.8110\n",
            "Epoch 83/100\n",
            "12400/12400 [==============================] - 1s 58us/sample - loss: 0.1675 - accuracy: 0.9457 - val_loss: 0.4028 - val_accuracy: 0.8160\n",
            "Epoch 84/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.1650 - accuracy: 0.9477 - val_loss: 0.3982 - val_accuracy: 0.8180\n",
            "Epoch 85/100\n",
            "12400/12400 [==============================] - 1s 58us/sample - loss: 0.1638 - accuracy: 0.9471 - val_loss: 0.3992 - val_accuracy: 0.8180\n",
            "Epoch 86/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.1619 - accuracy: 0.9474 - val_loss: 0.4004 - val_accuracy: 0.8200\n",
            "Epoch 87/100\n",
            "12400/12400 [==============================] - 1s 60us/sample - loss: 0.1598 - accuracy: 0.9489 - val_loss: 0.4019 - val_accuracy: 0.8200\n",
            "Epoch 88/100\n",
            "12400/12400 [==============================] - 1s 60us/sample - loss: 0.1577 - accuracy: 0.9498 - val_loss: 0.4066 - val_accuracy: 0.8130\n",
            "Epoch 89/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.1562 - accuracy: 0.9502 - val_loss: 0.4036 - val_accuracy: 0.8170\n",
            "Epoch 90/100\n",
            "12400/12400 [==============================] - 1s 60us/sample - loss: 0.1545 - accuracy: 0.9498 - val_loss: 0.4068 - val_accuracy: 0.8180\n",
            "Epoch 91/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.1531 - accuracy: 0.9500 - val_loss: 0.4081 - val_accuracy: 0.8120\n",
            "Epoch 92/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.1515 - accuracy: 0.9519 - val_loss: 0.4070 - val_accuracy: 0.8170\n",
            "Epoch 93/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.1493 - accuracy: 0.9513 - val_loss: 0.4088 - val_accuracy: 0.8160\n",
            "Epoch 94/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.1477 - accuracy: 0.9520 - val_loss: 0.4092 - val_accuracy: 0.8160\n",
            "Epoch 95/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.1462 - accuracy: 0.9526 - val_loss: 0.4110 - val_accuracy: 0.8150\n",
            "Epoch 96/100\n",
            "12400/12400 [==============================] - 1s 58us/sample - loss: 0.1445 - accuracy: 0.9528 - val_loss: 0.4133 - val_accuracy: 0.8100\n",
            "Epoch 97/100\n",
            "12400/12400 [==============================] - 1s 58us/sample - loss: 0.1436 - accuracy: 0.9529 - val_loss: 0.4155 - val_accuracy: 0.8130\n",
            "Epoch 98/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.1421 - accuracy: 0.9534 - val_loss: 0.4151 - val_accuracy: 0.8130\n",
            "Epoch 99/100\n",
            "12400/12400 [==============================] - 1s 60us/sample - loss: 0.1402 - accuracy: 0.9548 - val_loss: 0.4158 - val_accuracy: 0.8170\n",
            "Epoch 100/100\n",
            "12400/12400 [==============================] - 1s 59us/sample - loss: 0.1390 - accuracy: 0.9545 - val_loss: 0.4172 - val_accuracy: 0.8190\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CI6kwmF8IUMC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "40c2cc89-2e1a-41db-8ea0-0e12c70a9c0f"
      },
      "cell_type": "code",
      "source": [
        "size = 0\n",
        "for i in test_y:\n",
        "  if i == 1:\n",
        "    size += 1\n",
        "print(size*1.0/len(test_y)*100)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50.34848484848485\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JsfcWGhBDF7L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c1ca4286-4b4c-440d-d35c-5907b7d1a08d"
      },
      "cell_type": "code",
      "source": [
        "# 验证模型\n",
        "\n",
        "result = dmsc_model.evaluate(test_x, test_y)\n",
        "print(result)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6600/6600 [==============================] - 0s 40us/sample - loss: 0.4826 - accuracy: 0.8086\n",
            "[0.4826192412231908, 0.80863637]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vbNnsU_gqgyL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "2b508605-d19b-4859-c0ab-a927e33af6be"
      },
      "cell_type": "code",
      "source": [
        "dmsc_model.predict(test_x)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.92586744],\n",
              "       [0.8183203 ],\n",
              "       [0.9931318 ],\n",
              "       ...,\n",
              "       [0.4850828 ],\n",
              "       [0.0115703 ],\n",
              "       [0.38390815]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "Sw5yNGx-sHf9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "db076bc8-db29-4967-8cb0-ace49ff473db"
      },
      "cell_type": "code",
      "source": [
        "text = '还不错咯，值得一看'\n",
        "text = seg.cut(text)\n",
        "number = [word_index.get(i,-1) for i in text]\n",
        "print(number)\n",
        "number = np.array([number])\n",
        "number = k.preprocessing.sequence.pad_sequences(number,\n",
        "                                                maxlen=256,\n",
        "                                                padding='post',\n",
        "                                                value=0\n",
        "                                                )\n",
        "print(number)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[149, 752, 3577, -1, 221, 81, 474]\n",
            "[[ 149  752 3577   -1  221   81  474    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HolYVZoUDbjq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "a9293e4d-9aef-4684-8eb9-d1628248a084"
      },
      "cell_type": "code",
      "source": [
        "def text2number(text):\n",
        "  '''\n",
        "  将文字转换成为数值型\n",
        "  '''\n",
        "  text = seg.cut(text)\n",
        "  number = [word_index.get(i,1) for i in text]\n",
        "  number = np.array([number])\n",
        "  number = k.preprocessing.sequence.pad_sequences(number,\n",
        "                                                   maxlen=256,\n",
        "                                                   padding='post',\n",
        "                                                   value=0\n",
        "                                                   )\n",
        "  return number\n",
        "\n",
        "\n",
        "# 自嗨一下\n",
        "while True:\n",
        "  comment = input()\n",
        "  if comment == 'q':\n",
        "    print('测试结束')\n",
        "    break\n",
        "  comment_number = text2number(comment)\n",
        "  pred = dmsc_model.predict(comment_number)\n",
        "  if pred[0][0] > 0.5:\n",
        "    print('有好感')\n",
        "  else:\n",
        "    print('无好感')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "看毛线呀，这个还不如回家看\n",
            "无好感\n",
            "这是我今年看过最好看的电影了，点个赞\n",
            "有好感\n",
            "支持一波\n",
            "有好感\n",
            "情怀电影吧，看不看都行\n",
            "无好感\n",
            "特效爆炸好吗！\n",
            "有好感\n",
            "q\n",
            "测试结束\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WYxYXgXqvX3m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "到此为止，OK，我们使用了官方的教材，同时在官方教材中做了延伸，训练数据来源于豆瓣，么大～"
      ]
    },
    {
      "metadata": {
        "id": "XdAQyHDbSGRs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "## Github\n",
        "\n",
        "[深度学习——Tensorflow学习（三）文本分类.ipynb](https://github.com/JavanTang/Learn-a-little-tensorflow-every-day/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Tensorflow%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.ipynb)\n",
        "\n",
        "## Colaboratory\n",
        "[深度学习——Tensorflow学习（三）文本分类.ipynb](https://colab.research.google.com/drive/16FGkMgX4p6bQibwwIdzsC71NTX6hyOdU)\n",
        "\n",
        "## Reference\n",
        "\n",
        "1. [Tensorflow官方教程](https://www.tensorflow.org/tutorials/keras/basic_text_classification) \n",
        "2. [pad_sequences函数](https://blog.csdn.net/HHTNAN/article/details/82585776)\n",
        "3. [什么是激活函数](https://www.zhihu.com/question/22334626)\n",
        "4. [测试集与验证集的区别](https://www.zhihu.com/question/26588665)\n",
        "5. [知乎中对词向量的解释](https://www.zhihu.com/question/21714667)\n",
        "6. [Embedding](https://blog.csdn.net/wangyangzhizhou/article/details/77530479)\n",
        "7. [Google Colab 免费 GPU 使用教程](https://juejin.im/post/5c05e1bc518825689f1b4948)\n",
        "8. [过滤符号](https://blog.csdn.net/mach_learn/article/details/41744487)\n",
        "9. [中文影评数据集](https://www.kaggle.com/utmhikari/doubanmovieshortcomments)\n",
        "\n",
        "\n",
        "下班了还没有撸完这篇文章，明天在这个文章下面更新使用新浪提供的数据做中文的文本分类。"
      ]
    },
    {
      "metadata": {
        "id": "eyKU6FtSLtDl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3e53Fa-UoXVJ",
        "colab_type": "code",
        "outputId": "e58a2002-5e39-469f-ec13-13d071bae5a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DMSC.csv  simplifyweibo_4_moods.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "psu-XJCxolfT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}